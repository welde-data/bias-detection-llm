{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf11595d",
   "metadata": {},
   "source": [
    "## setup paths + load prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3657aa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "PROMPTS_PATH = Path(\"prompts/prompts_v1.json\")\n",
    "OUTPUTS_DIR = Path(\"outputs\")\n",
    "OUTPUTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "prompts_cfg = json.loads(PROMPTS_PATH.read_text(encoding=\"utf-8\"))\n",
    "domain = prompts_cfg[\"domain\"]\n",
    "formulations = prompts_cfg[\"formulations\"]\n",
    "items = prompts_cfg[\"items\"]\n",
    "\n",
    "domain, list(formulations.keys()), len(items)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5508725",
   "metadata": {},
   "source": [
    "## Gemini client “hello” check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177def30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GEMINI_MODEL = os.getenv(\"GEMINI_MODEL\", \"gemini-2.5-flash\")\n",
    "\n",
    "assert GEMINI_API_KEY, \"Missing GEMINI_API_KEY in your .env\"\n",
    "\n",
    "from google import genai\n",
    "gemini_client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "\n",
    "def call_gemini(prompt: str) -> str:\n",
    "    resp = gemini_client.models.generate_content(\n",
    "        model=GEMINI_MODEL,\n",
    "        contents=prompt\n",
    "    )\n",
    "    return (resp.text or \"\").strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb68b6fc",
   "metadata": {},
   "source": [
    "## Second model via Ollama (local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07332ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def read_code(path: Path) -> str:\n",
    "    return path.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "\n",
    "def run_python_file(path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Executes a Python file in a subprocess and captures stdout/stderr.\n",
    "    Returns a dict with exit_code, stdout, stderr (traceback usually in stderr).\n",
    "    \"\"\"\n",
    "    result = subprocess.run(\n",
    "        [\"python\", str(path)],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    return {\n",
    "        \"exit_code\": result.returncode,\n",
    "        \"stdout\": result.stdout,\n",
    "        \"stderr\": result.stderr\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605ba9f3",
   "metadata": {},
   "source": [
    "## Prompt builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440733a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(code_snippet: str, traceback_text: str, stdout_text: str, exit_code: int) -> str:\n",
    "    return f\"\"\"\n",
    "You are a teaching assistant for beginner Python students.\n",
    "\n",
    "You must use ONLY the provided traceback/output.\n",
    "Do NOT guess the error type, do NOT invent line numbers, and do NOT invent code that is not shown.\n",
    "If information is missing, state what is missing and what to do next.\n",
    "\n",
    "Hard rules:\n",
    "- Do NOT provide full code solutions.\n",
    "- Do NOT output code blocks or fenced code (no ```).\n",
    "- Do NOT write a corrected version of the program.\n",
    "\n",
    "Inputs:\n",
    "Exit code: {exit_code}\n",
    "\n",
    "Stdout:\n",
    "{stdout_text}\n",
    "\n",
    "Stderr (traceback):\n",
    "{traceback_text}\n",
    "\n",
    "Student code:\n",
    "{code_snippet}\n",
    "\n",
    "Output format (use headings exactly):\n",
    "## Error summary\n",
    "## What it means\n",
    "## Why it happened here\n",
    "## Conceptual fix\n",
    "## Debugging questions\n",
    "\"\"\".strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b5ae1f",
   "metadata": {},
   "source": [
    "## Detect and sanitize code-like output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6438de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "CODE_FENCE_RE = re.compile(r\"```\")\n",
    "INLINE_CODE_RE = re.compile(r\"`[^`]+`\")  # inline backticks\n",
    "CODEY_LINE_RE = re.compile(\n",
    "    r\"^\\s*(def |class |import |from |for |while |try:|except |return\\b|raise\\b|with |print\\()\",\n",
    "    re.MULTILINE\n",
    ")\n",
    "\n",
    "def sanitize_llm_output(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Enforces: explanation-only, no code blocks, no code-like lines.\n",
    "    If the model output contains code markers, replace with a safe fallback.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        text = \"\"\n",
    "\n",
    "    # Normalize a bit (avoid weird spacing making detection harder)\n",
    "    cleaned = text.strip()\n",
    "\n",
    "    # Hard stops: fenced code blocks or code-like lines\n",
    "    if CODE_FENCE_RE.search(cleaned) or CODEY_LINE_RE.search(cleaned):\n",
    "        return fallback_explanation()\n",
    "\n",
    "    # Inline backticks often introduce code fragments; remove them.\n",
    "    cleaned = INLINE_CODE_RE.sub(lambda m: m.group(0).replace(\"`\", \"\"), cleaned)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def fallback_explanation() -> str:\n",
    "    \"\"\"\n",
    "    Generic, constraint-safe explanation to use when the model output violates rules.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        \"## Error summary\\n\"\n",
    "        \"I can’t include code or code-like output. Below is a conceptual explanation.\\n\\n\"\n",
    "        \"## What it means\\n\"\n",
    "        \"Python encountered something it could not parse or execute, and stopped.\\n\\n\"\n",
    "        \"## Why it happened here\\n\"\n",
    "        \"Use the last lines of the traceback to find the exact file and line where execution failed.\\n\"\n",
    "        \"That line (or the line just before it) typically contains the root cause.\\n\\n\"\n",
    "        \"## Conceptual fix\\n\"\n",
    "        \"- Identify the object mentioned in the error (variable, function, index, type).\\n\"\n",
    "        \"- Check it exists before use, and that it has the expected type/value.\\n\"\n",
    "        \"- If the error mentions an index/key, confirm the container actually contains it.\\n\"\n",
    "        \"- Make one small change at a time, then re-run to validate.\\n\\n\"\n",
    "        \"## What to check next\\n\"\n",
    "        \"- Which exact line number is referenced last in the traceback?\\n\"\n",
    "        \"- What are the types/values of the variables used on that line?\\n\"\n",
    "        \"- What assumption did the code make that might be false?\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baa5ee8",
   "metadata": {},
   "source": [
    "## Ask Gemini with prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1f1f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gemini(prompt: str) -> str:\n",
    "    resp = client.models.generate_content(\n",
    "        model=MODEL_NAME,\n",
    "        contents=prompt\n",
    "    )\n",
    "    return resp.text or \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bfa455",
   "metadata": {},
   "source": [
    "## Save one markdown report per error case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8db9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def write_markdown(case_name: str, run_info: dict, explanation: str) -> Path:\n",
    "    out_path = OUTPUTS_DIR / f\"{case_name}.md\"\n",
    "\n",
    "    tb = (run_info.get(\"stderr\") or \"\").strip() or \"(No stderr/traceback captured.)\"\n",
    "    out = (run_info.get(\"stdout\") or \"\").strip() or \"(No stdout captured.)\"\n",
    "    exit_code = run_info.get(\"exit_code\")\n",
    "\n",
    "    safe_expl = sanitize_llm_output(explanation)\n",
    "\n",
    "    md = (\n",
    "        f\"# {case_name}\\n\\n\"\n",
    "        f\"## Run info\\n\"\n",
    "        f\"- Exit code: {exit_code}\\n\\n\"\n",
    "        f\"## Stdout\\n\"\n",
    "        f\"```\\n{out}\\n```\\n\\n\"\n",
    "        f\"## Traceback\\n\"\n",
    "        f\"```\\n{tb}\\n```\\n\\n\"\n",
    "        f\"## Explanation\\n\"\n",
    "        f\"{safe_expl.strip()}\\n\"\n",
    "    )\n",
    "\n",
    "    out_path.write_text(md, encoding=\"utf-8\")\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b525573d",
   "metadata": {},
   "source": [
    "## Batch run all error scripts (generate 5 markdown files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_files = sorted(ERRORS_DIR.glob(\"*.py\"))\n",
    "assert error_files, \"No error files found in ./errors\"\n",
    "\n",
    "generated = []\n",
    "\n",
    "for path in error_files:\n",
    "    case_name = path.stem\n",
    "    code = read_code(path)\n",
    "    run_info = run_python_file(path)\n",
    "\n",
    "    # Gate: never call the LLM if we did not capture an actual error traceback\n",
    "    if run_info[\"exit_code\"] == 0 or not run_info[\"stderr\"].strip():\n",
    "        expl = (\n",
    "            \"## Error summary\\n\"\n",
    "            \"No traceback was captured, so I cannot identify the exact error.\\n\\n\"\n",
    "            \"## What it means\\n\"\n",
    "            \"A traceback is required to know which exception occurred and where it happened.\\n\\n\"\n",
    "            \"## Why it happened here\\n\"\n",
    "            \"Either the script did not raise an uncaught exception, or the failing line did not execute.\\n\\n\"\n",
    "            \"## Conceptual fix\\n\"\n",
    "            \"1. Ensure the script actually triggers the intended failing line when run.\\n\"\n",
    "            \"2. Remove any try/except that catches the error and prevents a traceback.\\n\"\n",
    "            \"3. Re-run and confirm a non-zero exit code and non-empty stderr.\\n\\n\"\n",
    "            \"## Debugging questions\\n\"\n",
    "            \"1. Does the line that should fail actually execute when the script runs?\\n\"\n",
    "            \"2. Is an exception being caught before it reaches the interpreter?\\n\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = build_prompt(\n",
    "            code_snippet=code,\n",
    "            traceback_text=run_info[\"stderr\"],\n",
    "            stdout_text=run_info[\"stdout\"],\n",
    "            exit_code=run_info[\"exit_code\"]\n",
    "        )\n",
    "        raw_expl = ask_gemini(prompt)\n",
    "        expl = sanitize_llm_output(raw_expl)\n",
    "\n",
    "    out_path = write_markdown(case_name, run_info, expl)\n",
    "    generated.append(out_path)\n",
    "\n",
    "generated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce341e42",
   "metadata": {},
   "source": [
    "## Quick compliance check over outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e275a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explanation_section_has_code_fence(md_text: str) -> bool:\n",
    "    if \"## Explanation\" not in md_text:\n",
    "        return False\n",
    "    expl = md_text.split(\"## Explanation\", 1)[1]\n",
    "    return \"```\" in expl\n",
    "\n",
    "violations = []\n",
    "for p in OUTPUTS_DIR.glob(\"*.md\"):\n",
    "    txt = p.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "    if explanation_section_has_code_fence(txt):\n",
    "        violations.append(p.name)\n",
    "\n",
    "violations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6ed1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "p = Path(\"outputs/case_03_name_error.md\")\n",
    "print(p.read_text(encoding=\"utf-8\", errors=\"replace\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
